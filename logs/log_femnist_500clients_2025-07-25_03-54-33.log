
================================================================================
FEDERATED LEARNING BENCHMARK - 2025-07-25T03:58:12.630210
Dataset: femnist
Number of clients: 500
Batch size: 32
Local epochs: 5
Desired accuracy: 0.9000
Maximum rounds: 1200
Device in use: GPU
  GPU per client: 2.0
[92mINFO [0m:      Starting Flower ServerApp, config: num_rounds=1200, no round_timeout
[92mINFO [0m:      
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Using initial global parameters provided by strategy
[92mINFO [0m:      Starting evaluation of initial global parameters
Round 0 at 54.675162 seconds - Server-side evaluation: loss 4.3544, accuracy 0.0504
[92mINFO [0m:      initial parameters (loss, other metrics): 4.354351051101984, {'accuracy': 0.050369651716854154}
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 500 clients (out of 500)
[92mINFO [0m:      aggregate_fit: received 500 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
Round 1 at 2901.638224 seconds - Server-side evaluation: loss 5.2780, accuracy 0.0544
[92mINFO [0m:      fit progress: (1, 5.278026316442284, {'accuracy': 0.054391609765682566}, 2846.96805668)
[92mINFO [0m:      configure_evaluate: strategy sampled 250 clients (out of 500)
[92mINFO [0m:      aggregate_evaluate: received 250 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 500 clients (out of 500)
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[33m(raylet)[0m [2025-07-25 05:11:14,410 E 5789 5789] (raylet) node_manager.cc:3064: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e, IP: 10.120.17.181) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.120.17.181`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
<<<<<<< HEAD
=======

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 67f4d4074df7fc3c15e7a744fd8e906f94ae38ddd621696f0132336e) where the task (actor ID: 5845c80d6fd0bd4f531e531701000000, name=ClientAppActor.__init__, pid=7687, memory used=1.07GB) was running was 29.87GB / 31.38GB (0.951926), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-ef81c85acdf77a931489773d0fbadd8d13f1a7a704d9338677ec45ce*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
5573	23.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
7687	1.07	ray::ClientAppActor.run
2033	0.84	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node --dn...
2444	0.60	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
2709	0.54	python -u federated_learning.py --dataset femnist --num_clients 500 --local_epochs 5 --desired_accur...
5644	0.18	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/sess...
5789	0.06	/home/dd/.local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tm...
1982	0.06	/home/dd/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/node /hom...
5738	0.05	/home/dd/miniconda3/envs/fl_bench/bin/python /home/dd/.local/lib/python3.10/site-packages/ray/dashbo...
5863	0.04	/home/dd/miniconda3/envs/fl_bench/bin/python -u /home/dd/.local/lib/python3.10/site-packages/ray/das...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
