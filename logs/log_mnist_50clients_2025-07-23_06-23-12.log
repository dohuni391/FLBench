
================================================================================
FEDERATED LEARNING BENCHMARK - 2025-07-23T06:23:15.777216
Dataset: mnist
Number of clients: 50
Batch size: 32
Local epochs: 5
Desired accuracy: 0.9000
Maximum rounds: 500
Device in use: GPU
  GPU per client: 1.0
[92mINFO [0m:      Starting Flower ServerApp, config: num_rounds=500, no round_timeout
[92mINFO [0m:      
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Using initial global parameters provided by strategy
[92mINFO [0m:      Starting evaluation of initial global parameters
Round 0 - Server-side evaluation: loss 2.5870, accuracy 0.1008
[92mINFO [0m:      initial parameters (loss, other metrics): 2.587039128621419, {'accuracy': 0.10075}
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 50 clients (out of 50)
[92mINFO [0m:      aggregate_fit: received 50 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
Round 1 - Server-side evaluation: loss 6.4902, accuracy 0.1130
[92mINFO [0m:      fit progress: (1, 6.49016180674235, {'accuracy': 0.113}, 149.2821236910022)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 50)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 50 clients (out of 50)
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[92mINFO [0m:      aggregate_fit: received 36 results and 14 failures
[33m(raylet)[0m [2025-07-23 06:28:17,545 E 174987 174987] (raylet) node_manager.cc:3064: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d, IP: 10.120.17.181) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.120.17.181`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
Round 2 - Server-side evaluation: loss 2.8515, accuracy 0.2774
[92mINFO [0m:      fit progress: (2, 2.8515403407414754, {'accuracy': 0.27741666666666664}, 293.3961449069975)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 50)
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[92mINFO [0m:      aggregate_evaluate: received 1 results and 24 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 50 clients (out of 50)
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[92mINFO [0m:      aggregate_fit: received 1 results and 49 failures
Round 3 - Server-side evaluation: loss 0.7379, accuracy 0.8279
[92mINFO [0m:      fit progress: (3, 0.7378831363320351, {'accuracy': 0.8279166666666666}, 309.0031797709962)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 50)
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[92mINFO [0m:      aggregate_evaluate: received 1 results and 24 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 50 clients (out of 50)
[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     An exception was raised when processing a message by RayBackend
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
  File "/home/dd/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 480, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/dd/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.120.17.181, ID: 91ac20a51bacab21bdbda58c43c2ab1c0d4243bf020fda88eca2300d) where the task (actor ID: 25fc9e5e094ff0a0b969a20301000000, name=ClientAppActor.__init__, pid=177110, memory used=1.03GB) was running was 29.93GB / 31.38GB (0.953703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.120.17.181`. To see the logs of the worker, use `ray logs worker-01d9bc2e70bdf6f122cfab1939667065220df9e69ccaf33b08e63f7a*out -ip 10.120.17.181. Top 10 memory users:
PID	MEM(GB)	COMMAND
174708	4.41	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
116735	4.12	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
149638	2.49	python -u federated_learning.py --dataset mnist --num_clients 50 --local_epochs 5 --desired_accuracy...
155790	1.42	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
80762	1.39	python federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy 0....
99207	1.38	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
30718	1.31	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
3462	1.15	python federated_learning.py --dataset mnist --num_clients 2 --local_epochs 1 --desired_accuracy 0.8...
94584	1.12	python -u federated_learning.py --dataset mnist --num_clients 10 --local_epochs 5 --desired_accuracy...
177109	1.04	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

